{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "- write function to get soup from each page (i.e. 10 results) [x]\n",
    "    - for each article, extract: title, link, year, site, first author [x]\n",
    "    - NB if returns empty page, stop the for loop\n",
    "- do this for 3 links: MRz, MRs, BC\n",
    "    - exclude all articles that are not in BC (all papers citing THE paper but are not actually on BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-17-be2018a3aed3>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-be2018a3aed3>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    return df_list\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "def scrape_url(url):\n",
    "    # get header: http://myhttpheader.com/\n",
    "    headers = {'User-Agent':\n",
    "           'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.115 Safari/537.36'}\n",
    "\n",
    "    seconds =  round((30-5)*np.random.random()+5)\n",
    "    print(\"Sleeping {} seconds to avoid bot detection\".format(seconds))\n",
    "    time.sleep(seconds) #from 5 to 30 seconds\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    \n",
    "    if \"Our systems have detected unusual traffic from your computer network\" in str(soup):\n",
    "        print (\"Google has blocked this computer for a short time because it has detected this scraping script.\")\n",
    "        break \n",
    "        \n",
    "    df_list = parse_soup(soup)\n",
    "    return df_list\n",
    "    \n",
    "def parse_soup(soup):\n",
    "    df_list = []\n",
    "    \n",
    "    num_articles = len(soup.select('[data-lid]'))\n",
    "    \n",
    "    for item in soup.select('[data-lid]'):\n",
    "        print(item)\n",
    "        try:\n",
    "            # extarct from HTML\n",
    "            title = [item.select('h3')[0].get_text()]\n",
    "            title = [x.replace('[HTML][HTML] ', \"\") for x in title]\n",
    "            link = [item.select('a')[0]['href']]\n",
    "            abstract = [item.select('.gs_rs')[0].get_text()]\n",
    "            info = item.select('.gs_a')[0].get_text()\n",
    "\n",
    "            # parse info\n",
    "            info_split = x.split(\" - \")\n",
    "            first_author = [info_split[0].split(\", \")[0]]\n",
    "            journal = info_split[1].split(\", \")\n",
    "            website = [info_split[2]]\n",
    "            \n",
    "            if len(journal) == 2:\n",
    "                journal_name = [journal[0]]\n",
    "                journal_year = [journal[1]]\n",
    "            else:\n",
    "                journal_year = [journal[0]]\n",
    "                journal_name = [None]\n",
    "            \n",
    "            article = [*title, *journal_year , *first_author, *website *journal_name, *link]\n",
    "            article = [x.strip(' ') for x in article]\n",
    "            article = [x.replace('\\xa0', \"\") for x in article]\n",
    "\n",
    "        except Exception as e:\n",
    "            #raise e \n",
    "            print('==cant parse this soup')\n",
    "            # trying to catch where this is failing\n",
    "            #print(item)\n",
    "            #print(item.select('.gs_a')[0].get_text())\n",
    "            #info_split = info.split(\"- \")\n",
    "            #print(info_split)\n",
    "            article=[]\n",
    "            \n",
    "        df_list.append(article)  \n",
    "          \n",
    "    return(df_list)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping 11 seconds to avoid bot detection\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c78a9ca45337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://scholar.google.com/scholar?start=20&q=mendelian+randomization&hl=en&as_sdt=2005&sciodt=0,5&cites=6613969377995102680&scipsc=1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df = pd.DataFrame(df_list, columns=['title', 'year', 'first_author', 'website', 'journal_name', 'link'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-88fd16d84897>\u001b[0m in \u001b[0;36mscrape_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Our systems have detected unusual traffic from your computer network\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Google has blocked this computer for a short time because it has detected this scraping script.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "# test one\n",
    "url='https://scholar.google.com/scholar?start=20&q=mendelian+randomization&hl=en&as_sdt=2005&sciodt=0,5&cites=6613969377995102680&scipsc=1'\n",
    "df_list = scrape_url(url)\n",
    "df_list\n",
    "#df = pd.DataFrame(df_list, columns=['title', 'year', 'first_author', 'website', 'journal_name', 'link'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from 0 to 9\n",
      "Results from 10 to 19\n",
      "Results from 20 to 29\n",
      "Results from 30 to 39\n",
      "Results from 40 to 49\n",
      "Results from 50 to 59\n",
      "Results from 60 to 69\n",
      "Results from 70 to 79\n",
      "Results from 80 to 89\n",
      "Results from 90 to 99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>first_author</th>\n",
       "      <th>source_site</th>\n",
       "      <th>source_name</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, year, first_author, source_site, source_name, link]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape 10 pages by 10 results in each\n",
    "total_df = pd.DataFrame()\n",
    "for i in range(0, 100, 10):\n",
    "    print(\"Results from {} to {}\".format(i, i+9))\n",
    "    url = 'https://scholar.google.com/scholar?start={}&q=mendelian+randomization&hl=en&as_sdt=2005&sciodt=0,5&cites=6613969377995102680&scipsc=1'.format(i)\n",
    "    #print(url)\n",
    "    df_list = scrape_url(url)\n",
    "    df = pd.DataFrame(df_list, columns=['title', 'year', 'first_author', 'website', 'journal_name', 'link'])\n",
    "    total_df = pd.concat([total_df,df])\n",
    "total_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_df.to_csv(\"parsing_MRz.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
