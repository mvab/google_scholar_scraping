{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "- write function to get soup from each page (i.e. 10 results) [x]\n",
    "    - for each article, extract: title, link, year, site, first author [x]\n",
    "    - NB if returns empty page, stop the for loop\n",
    "- do this for 3 links: MRz, MRs, BC\n",
    "    - exclude all articles that are not in BC (all papers citing THE paper but are not actually on BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url):\n",
    "    # get header: http://myhttpheader.com/\n",
    "    headers = {'User-Agent':\n",
    "           'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.115 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    \n",
    "  \n",
    "    df_list = parse_soup(soup)\n",
    "    return df_list\n",
    "    \n",
    "def parse_soup(soup):\n",
    "    df_list = []\n",
    "    \n",
    "    num_articles = len(soup.select('[data-lid]'))\n",
    "    \n",
    "    for item in soup.select('[data-lid]'):\n",
    "        try:\n",
    "            # extarct from HTML\n",
    "            title = [item.select('h3')[0].get_text()]\n",
    "            title = [x.replace('[HTML][HTML] ', \"\") for x in title]\n",
    "            link = [item.select('a')[0]['href']]\n",
    "            abstract = [item.select('.gs_rs')[0].get_text()]\n",
    "            info = item.select('.gs_a')[0].get_text()\n",
    "            \n",
    "            # parse info\n",
    "            info_split = x.split(\" - \")\n",
    "            first_author = [info_split[0].split(\", \")[0]]\n",
    "            journal = info_split[1].split(\", \")\n",
    "            website = [info_split[2]]\n",
    "            if len(journal) == 2:\n",
    "                journal_name = [journal[0]]\n",
    "                journal_year = [journal[1]]\n",
    "            else:\n",
    "                journal_year = [journal[0]]\n",
    "                journal_name = [None]\n",
    "            \n",
    "            article = [*title, *journal_year , *first_author, *website *journal_name, *link]\n",
    "            article = [x.strip(' ') for x in article]\n",
    "            article = [x.replace('\\xa0', \"\") for x in article]\n",
    "\n",
    "        except Exception as e:\n",
    "            #raise e \n",
    "            print('==cant parse this soup')\n",
    "            # trying to catch where this is failing\n",
    "            print(item)\n",
    "            print(item.select('.gs_a')[0].get_text())\n",
    "            #info_split = info.split(\"- \")\n",
    "            #print(info_split)\n",
    "            article=[]\n",
    "            \n",
    "        df_list.append(article)  \n",
    "          \n",
    "    return(df_list)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>first_author</th>\n",
       "      <th>website</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, year, first_author, website, journal_name, link]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one\n",
    "url='https://scholar.google.com/scholar?start=20&q=mendelian+randomization&hl=en&as_sdt=2005&sciodt=0,5&cites=6613969377995102680&scipsc=1'\n",
    "df_list = scrape_url(url)\n",
    "df_list\n",
    "df = pd.DataFrame(df_list, columns=['title', 'year', 'first_author', 'website', 'journal_name', 'link'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from 0 to 9\n",
      "Results from 10 to 19\n",
      "Results from 20 to 29\n",
      "Results from 30 to 39\n",
      "Results from 40 to 49\n",
      "Results from 50 to 59\n",
      "Results from 60 to 69\n",
      "Results from 70 to 79\n",
      "Results from 80 to 89\n",
      "Results from 90 to 99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>first_author</th>\n",
       "      <th>source_site</th>\n",
       "      <th>source_name</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, year, first_author, source_site, source_name, link]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape 10 pages by 10 results in each\n",
    "total_df = pd.DataFrame()\n",
    "for i in range(0, 100, 10):\n",
    "    print(\"Results from {} to {}\".format(i, i+9))\n",
    "    url = 'https://scholar.google.com/scholar?start={}&q=mendelian+randomization&hl=en&as_sdt=2005&sciodt=0,5&cites=6613969377995102680&scipsc=1'.format(i)\n",
    "    #print(url)\n",
    "    df_list = scrape_url(url)\n",
    "    df = pd.DataFrame(df_list, columns=['title', 'year', 'first_author', 'website', 'journal_name', 'link'])\n",
    "    total_df = pd.concat([total_df,df])\n",
    "total_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
